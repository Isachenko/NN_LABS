{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 лаба. Используем тут адаптивный парог"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тут создаем небольшие утилиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['shuffle', 'exp', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from math import exp\n",
    "import math\n",
    "\n",
    "def F_threshold_binary(s):\n",
    "    y = 0 if s <=0 else 1\n",
    "    return y\n",
    "\n",
    "def F_threshold_bipolar(s):\n",
    "    y = -1 if s<=0 else 1\n",
    "    return y\n",
    "\n",
    "def F_sigmoidal_binory(s, param = 1):\n",
    "    e = 0\n",
    "    if (s < -600 ) :\n",
    "        e = 9999999999\n",
    "    else :\n",
    "        e = exp(-param * s)\n",
    "    y = 1 / (1+(e))\n",
    "    return y\n",
    "    \n",
    "def F_sigmidal_bipolar(s, param = 1):\n",
    "    y = (2 / (1+(exp(-param * s)))) - 1;\n",
    "    return y\n",
    "\n",
    "def F_hiperbolic_tg(s):\n",
    "    return F_sigmidal_bipolar(s, 2)\n",
    "\n",
    "def F_linear(s):\n",
    "    return s\n",
    "\n",
    "def step_constant_small(_):\n",
    "    return 0.01\n",
    "\n",
    "def step_constant_medium(_):\n",
    "    return 0.1\n",
    "\n",
    "def step_constant_large(_):\n",
    "    return 0.5\n",
    "\n",
    "def step_adaptive(x):\n",
    "    s = sum(xi**2 for xi in x)\n",
    "    return 1.0 / (1 + s)\n",
    "\n",
    "#import math\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random, shuffle\n",
    "\n",
    "def generate_sin_dots(a, b, c, size, step):    \n",
    "    points = []\n",
    "    t = 0\n",
    "    for i in range(size):\n",
    "        t = t + step\n",
    "        y = a * math.sin(b * t) + c;\n",
    "        points.append((t, y))\n",
    "        \n",
    "    return points\n",
    "\n",
    "def generate_complex_func_dots(a, b, c, d, size, step):    \n",
    "    points = []\n",
    "    t = 0\n",
    "    for i in range(size):\n",
    "        t = t + step\n",
    "        y = a * math.cos(b * t) + c*math.sin(d * t);\n",
    "        points.append((t, y))\n",
    "        \n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тут определяем нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class perceptrone:\n",
    "    def __init__(self, inCount):\n",
    "        self.in_count = inCount\n",
    "        self.w = []\n",
    "        self.T = 1;\n",
    "        self.alf = 0.01\n",
    "            \n",
    "    def teach(self, inputs, outputs, Ee):\n",
    "        self.w = [0.1 for x in range(self.in_count)]\n",
    "        cur_epoch = 0\n",
    "        good_epoch = False\n",
    "        prevE = -1\n",
    "        while(not good_epoch):\n",
    "            j = 0\n",
    "            for cur_input in inputs:\n",
    "                self.alf = step_adaptive(cur_input)\n",
    "                S = 0\n",
    "                i = 0\n",
    "                for xi in cur_input:\n",
    "                    S = S + xi*self.w[i]\n",
    "                    i = i + 1;\n",
    "                S = S - self.T\n",
    "                y = F_linear(S)\n",
    "                K = y - outputs[j]\n",
    "                i = 0\n",
    "                for wi in self.w:\n",
    "                    wi = wi - (self.alf * cur_input[i] * K)\n",
    "                    self.w[i] = wi\n",
    "                    i = i + 1;\n",
    "                self.T = self.T + (self.alf * K)\n",
    "                j = j + 1\n",
    "            Es = 0\n",
    "            j = 0\n",
    "            for cur_input in inputs:\n",
    "                S = 0\n",
    "                i = 0\n",
    "                for xi in cur_input:\n",
    "                    S = S + xi*self.w[i]\n",
    "                    i = i + 1;\n",
    "                S = S - self.T\n",
    "                y = F_linear(S)\n",
    "                Es = Es + (y - outputs[j])**2\n",
    "            Es = Es / 2\n",
    "            if (prevE != -1) and (abs(prevE - Es) < Ee):\n",
    "                good_epoch = True\n",
    "            prevE = Es\n",
    "            cur_epoch = cur_epoch + 1\n",
    "            #if (good_epoch):\n",
    "            #    break\n",
    "#             if (cur_epoch > 200):\n",
    "#                 break\n",
    "        \n",
    "    def get_prophecy(self, sample):\n",
    "        i = 0\n",
    "        S = 0\n",
    "        for xi in sample:\n",
    "            S = S + xi*self.w[i]\n",
    "            i = i + 1;\n",
    "        S = S - self.T\n",
    "        y = F_linear(S)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class two_layer_perceptrone :\n",
    "    def __init__(self, in_count, interm_count) :\n",
    "        self.in_count = in_count\n",
    "        self.interm_count = interm_count\n",
    "        self.T_interm = [];\n",
    "        for i in range(interm_count) :\n",
    "            self.T_interm.append(0.1)\n",
    "        self.T_out = 0.5;\n",
    "        self.w_interm = []\n",
    "        \n",
    "    def get_prophecy(self, sample):\n",
    "        # forward interm layer\n",
    "        y = []\n",
    "        for j in range(self.interm_count) :\n",
    "            S = 0\n",
    "            i = 0\n",
    "            for xi in sample:\n",
    "                S = S + xi*self.w_interm[i][j]\n",
    "                i = i + 1;\n",
    "            S = S - self.T_interm[j]\n",
    "            y.append(F_sigmoidal_binory(S))\n",
    "\n",
    "        #forward out layer\n",
    "        S = 0\n",
    "        i = 0\n",
    "        for yi in y:\n",
    "            S = S + yi*self.w_out[i]\n",
    "            i = i + 1;\n",
    "        S = S - self.T_out\n",
    "        Y = F_linear(S)\n",
    "\n",
    "        return Y\n",
    "    \n",
    "    def teach(self, inputs, outputs, Ee) :\n",
    "        for i in range(self.in_count) :\n",
    "            wi = [0.1 for x in range(self.interm_count)]\n",
    "            self.w_interm.append(wi)\n",
    "            \n",
    "            \n",
    "        self.w_out = [0.1 for x in range(self.interm_count)]\n",
    "        \n",
    "        \n",
    "        cur_epoch = 0\n",
    "        good_epoch = False\n",
    "        prevE = -1\n",
    "        while(not good_epoch):\n",
    "        \n",
    "            sample_index = 0\n",
    "            for cur_input in inputs:\n",
    "                y = []\n",
    "\n",
    "                # forward interm layer\n",
    "                for j in range(self.interm_count) :\n",
    "                    S = 0\n",
    "                    i = 0\n",
    "                    for xi in cur_input:\n",
    "                        S = S + xi*self.w_interm[i][j]\n",
    "                        i = i + 1;\n",
    "                    S = S - self.T_interm[j]\n",
    "                    y.append(F_sigmoidal_binory(S))\n",
    "\n",
    "                #forward out layer\n",
    "                S = 0\n",
    "                i = 0\n",
    "                for yi in y:\n",
    "                    S = S + yi*self.w_out[i]\n",
    "                    i = i + 1;\n",
    "                S = S - self.T_out\n",
    "                Y = F_linear(S)\n",
    "\n",
    "                #count out layer E\n",
    "                E_out = Y - outputs[sample_index]\n",
    "\n",
    "                #count interm layer E\n",
    "                E_interm = []\n",
    "                for i in range(self.interm_count):\n",
    "                    E_interm.append(self.w_out[i] * E_out)\n",
    "\n",
    "                #count in_layer E\n",
    "    #             E_in = []\n",
    "    #             for i in range(self.in_count):\n",
    "    #                 E_in[i].append(0)\n",
    "    #                 for j in range(self.interm_count):\n",
    "    #                     E_in[i] = E_in[i] + E_interm[j]*self.w_interm[i][j]*(2 * (1 - y[j]) * y[j])\n",
    "\n",
    "        \n",
    "                #count adaptive alphas\n",
    "                alf_interm = step_adaptive(cur_input)\n",
    "                alf_out = step_adaptive(y)\n",
    "                \n",
    "                # count new weights for interm layer\n",
    "                for j in range(self.interm_count) :\n",
    "                    for i in range(self.in_count) :\n",
    "                        self.w_interm[i][j] = self.w_interm[i][j] - alf_interm* \\\n",
    "                        (E_interm[j]) * ((1 - y[j]) * y[j]) * cur_input[i]\n",
    "\n",
    "                    self.T_interm[j] = self.T_interm[j] + alf_interm * \\\n",
    "                    (E_interm[j]) * ((1 - y[j]) * y[j])\n",
    "\n",
    "                    #count new weights for out layer\n",
    "                    self.w_out[j] = self.w_out[j] - alf_out * (E_out) * y[j]\n",
    "                #count new weight for out thershold\n",
    "                self.T_out = self.T_out + alf_out * (E_out)\n",
    "\n",
    "                sample_index = sample_index + 1\n",
    "            \n",
    "            # count Es\n",
    "            Es = 0\n",
    "            j = 0\n",
    "            for cur_input in inputs:\n",
    "                ans = self.get_prophecy(cur_input)\n",
    "                Es = Es + (ans - outputs[j])**2\n",
    "                j = j + 1\n",
    "            Es = Es / 2\n",
    "            print Es\n",
    "            if (prevE != -1) and (Es < Ee) and (cur_epoch > 3):\n",
    "                print cur_epoch\n",
    "                good_epoch = True\n",
    "                \n",
    "#             if (cur_epoch > -1) :\n",
    "#                 print cur_epoch\n",
    "#                 return\n",
    "                \n",
    "            prevE = Es\n",
    "            cur_epoch = cur_epoch + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тут решаем задачу используя определенную выше сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802593650871\n",
      "0.802939140155\n",
      "0.803310822454\n",
      "0.803706116908\n",
      "0.8041224864\n",
      "0.804557457861\n",
      "0.805008638327\n",
      "0.805473727293\n",
      "0.805950525886\n",
      "0.806436943306\n",
      "0.806931000959\n",
      "0.807430834655\n",
      "0.807934695185\n",
      "0.808440947573\n",
      "0.808948069247\n",
      "0.80945464734\n",
      "0.809959375312\n",
      "0.810461049044\n",
      "0.810958562535\n",
      "0.811450903327\n",
      "0.81193714774\n",
      "0.812416456001\n",
      "0.812888067331\n",
      "0.813351295046\n",
      "0.813805521712\n",
      "0.814250194384\n",
      "0.814684819973\n",
      "0.81510896074\n",
      "0.815522229948\n",
      "0.815924287672\n",
      "0.81631483679\n",
      "0.816693619133\n",
      "0.817060411829\n",
      "0.817415023803\n",
      "0.817757292467\n",
      "0.818087080563\n",
      "0.818404273179\n",
      "0.818708774918\n",
      "0.819000507217\n",
      "0.819279405806\n",
      "0.819545418307\n",
      "0.819798501953\n",
      "0.820038621433\n",
      "0.820265746835\n",
      "0.820479851704\n",
      "0.820680911179\n",
      "0.820868900225\n",
      "0.821043791937\n",
      "0.821205555904\n",
      "0.821354156647\n",
      "0.821489552094\n",
      "0.821611692104\n",
      "0.821720517026\n",
      "0.821815956284\n",
      "0.821897926975\n",
      "0.821966332491\n",
      "0.82202106113\n",
      "0.822061984705\n",
      "0.822088957151\n",
      "0.822101813093\n",
      "0.822100366399\n",
      "0.822084408685\n",
      "0.822053707783\n",
      "0.822008006144\n",
      "0.821947019184\n",
      "0.821870433555\n",
      "0.821777905327\n",
      "0.821669058085\n",
      "0.821543480918\n",
      "0.821400726292\n",
      "0.821240307796\n",
      "0.821061697756\n",
      "0.820864324686\n",
      "0.820647570589\n",
      "0.82041076806\n",
      "0.820153197216\n",
      "0.819874082395\n",
      "0.819572588645\n",
      "0.819247817955\n",
      "0.818898805231\n",
      "0.818524513981\n",
      "0.818123831701\n",
      "0.817695564926\n",
      "0.817238433932\n",
      "0.816751067064\n",
      "0.816231994659\n",
      "0.81567964253\n",
      "0.815092325003\n",
      "0.814468237453\n",
      "0.81380544832\n",
      "0.81310189057\n",
      "0.812355352566\n",
      "0.81156346831\n",
      "0.810723707036\n",
      "0.809833362089\n",
      "0.808889539091\n",
      "0.807889143322\n",
      "0.806828866314\n",
      "0.805705171601\n",
      "0.804514279612\n",
      "0.803252151672\n",
      "0.801914473104\n",
      "0.800496635403\n",
      "0.798993717494\n",
      "0.797400466084\n",
      "0.795711275119\n",
      "0.793920164406\n",
      "0.792020757467\n",
      "0.790006258702\n",
      "0.787869430026\n",
      "0.785602567118\n",
      "0.783197475539\n",
      "0.780645446989\n",
      "0.777937236071\n",
      "0.77506303802\n",
      "0.772012467957\n",
      "0.768774542354\n",
      "0.765337663573\n",
      "0.761689608488\n",
      "0.757817522472\n",
      "0.753707920245\n",
      "0.749346695421\n",
      "0.744719140967\n",
      "0.739809983209\n",
      "0.734603432563\n",
      "0.729083254763\n",
      "0.723232867109\n",
      "0.717035465055\n",
      "0.710474185464\n",
      "0.703532313952\n",
      "0.696193544986\n",
      "0.688442304819\n",
      "0.680264148863\n",
      "0.671646246707\n",
      "0.662577969663\n",
      "0.653051597331\n",
      "0.643063161074\n",
      "0.632613443403\n",
      "0.62170915267\n",
      "0.610364292004\n",
      "0.598601739591\n",
      "0.586455053709\n",
      "0.573970509848\n",
      "0.561209368073\n",
      "0.548250355847\n",
      "0.535192334149\n",
      "0.522157092208\n",
      "0.509292188148\n",
      "0.496773719078\n",
      "0.484808864943\n",
      "0.473638006625\n",
      "0.463536172101\n",
      "0.454813517386\n",
      "0.447814505522\n",
      "0.442915411758\n",
      "0.44051976279\n",
      "0.44105131971\n",
      "0.444944246493\n",
      "0.452630176982\n",
      "0.464522011609\n",
      "0.480994447225\n",
      "0.502361473305\n",
      "0.528851354825\n",
      "0.560579959375\n",
      "0.597523658231\n",
      "0.639493412882\n",
      "0.686112013187\n",
      "0.736796713062\n",
      "0.790749657315\n",
      "0.846958447235\n",
      "0.904208894971\n",
      "0.961111424825\n",
      "1.01614167993\n",
      "1.06769471573\n",
      "1.11415079169\n",
      "1.1539493511\n",
      "1.18566649365\n",
      "1.20809030631\n",
      "1.22028801848\n",
      "1.2216592215\n",
      "1.21197037711\n",
      "1.19136745122\n",
      "1.16036555216\n",
      "1.11981664745\n",
      "1.07085847358\n",
      "1.01484936285\n",
      "0.95329469694\n",
      "0.887770977663\n",
      "0.819853116266\n",
      "0.751049613778\n",
      "0.682749025908\n",
      "0.616179683937\n",
      "0.552383267298\n",
      "0.492201639835\n",
      "0.436275460513\n",
      "0.385052495805\n",
      "0.338803283597\n",
      "0.297641782861\n",
      "0.261548826737\n",
      "0.23039651095\n",
      "0.203972031246\n",
      "0.181999880098\n",
      "0.164161685519\n",
      "0.150113298469\n",
      "0.139498997351\n",
      "0.1319628756\n",
      "0.127157615149\n",
      "0.124750932726\n",
      "0.124430027482\n",
      "0.125904367654\n",
      "0.128907140433\n",
      "0.133195661072\n",
      "0.138551001094\n",
      "0.144777056203\n",
      "0.15169923559\n",
      "0.159162918042\n",
      "0.167031787954\n",
      "0.175186136452\n",
      "0.183521189624\n",
      "0.191945506927\n",
      "0.200379477894\n",
      "0.208753933662\n",
      "0.217008881173\n",
      "0.225092361562\n",
      "0.232959429782\n",
      "0.240571249546\n",
      "0.247894295843\n",
      "0.254899656275\n",
      "0.261562422096\n",
      "0.267861159893\n",
      "0.2737774552\n",
      "0.279295519895\n",
      "0.284401855848\n",
      "0.289084968019\n",
      "0.293335120908\n",
      "0.297144132925\n",
      "0.300505203945\n",
      "0.303412771881\n",
      "0.305862394678\n",
      "0.30785065464\n",
      "0.309375082419\n",
      "0.31043409841\n",
      "0.311026969639\n",
      "0.311153780488\n",
      "0.310815415929\n",
      "0.31001355607\n",
      "0.308750681062\n",
      "0.307030085545\n",
      "0.304855901912\n",
      "0.302233131798\n",
      "0.299167685251\n",
      "0.295666427099\n",
      "0.291737230045\n",
      "0.287389034039\n",
      "0.282631911458\n",
      "0.277477137602\n",
      "0.271937265936\n",
      "0.266026207482\n",
      "0.259759313624\n",
      "0.253153461524\n",
      "0.246227141177\n",
      "0.239000543002\n",
      "0.231495644677\n",
      "0.223736295726\n",
      "0.215748298126\n",
      "0.207559480977\n",
      "0.199199766949\n",
      "0.19070122796\n",
      "0.182098127174\n",
      "0.173426944063\n",
      "0.164726378902\n",
      "0.156037332683\n",
      "0.147402858042\n",
      "0.138868076407\n",
      "0.130480056261\n",
      "0.122287647074\n",
      "0.114341263269\n",
      "0.106692612495\n",
      "0.0993943625151\n",
      "0.0924997413108\n",
      "0.086062065514\n",
      "0.0801341931403\n",
      "0.0747678978213\n",
      "0.0700131634042\n",
      "0.065917399946\n",
      "0.0625245848158\n",
      "0.0598743358525\n",
      "0.0580009272899\n",
      "0.0569322634137\n",
      "0.0566888295619\n",
      "0.0572826449671\n",
      "0.0587162468726\n",
      "0.0609817400619\n",
      "0.0640599501094\n",
      "0.0679197219178\n",
      "0.0725174070608\n",
      "0.0777965836909\n",
      "0.0836880509224\n",
      "0.0901101353349\n",
      "0.0969693403392\n",
      "0.104161359533\n",
      "0.111572462938\n",
      "0.119081250473\n",
      "0.126560750706\n",
      "0.13388082559\n",
      "0.140910824519\n",
      "0.147522414647\n",
      "0.153592500293\n",
      "0.159006133419\n",
      "0.163659310782\n",
      "0.167461552084\n",
      "0.170338157877\n",
      "0.172232056072\n",
      "0.173105161441\n",
      "0.172939192521\n",
      "0.171735913867\n",
      "0.169516797014\n",
      "0.166322119315\n",
      "0.162209544275\n",
      "0.157252248633\n",
      "0.15153667893\n",
      "0.145160032736\n",
      "0.138227566526\n",
      "0.130849833384\n",
      "0.123139949546\n",
      "0.115210979929\n",
      "0.107173520291\n",
      "0.0991335384543\n",
      "0.0911905204386\n",
      "0.0834359504061\n",
      "0.0759521370153\n",
      "0.068811384001\n",
      "0.0620754900594\n",
      "0.0557955528635\n",
      "0.0500120443888\n",
      "0.0447551196885\n",
      "0.040045118637\n",
      "0.035893219677\n",
      "0.0323022058953\n",
      "0.0292673064392\n",
      "0.0267770799591\n",
      "0.0248143110825\n",
      "0.0233568955442\n",
      "0.0223786942633\n",
      "0.021850341148\n",
      "0.0217399935632\n",
      "0.02201401812\n",
      "0.0226376076649\n",
      "0.0235753280506\n",
      "0.0247915954564\n",
      "0.0262510867352\n",
      "0.0279190865251\n",
      "0.0297617757441\n",
      "0.0317464666351\n",
      "0.0338417897985\n",
      "0.0360178387093\n",
      "0.0382462770949\n",
      "0.0405004143091\n",
      "0.0427552535086\n",
      "0.0449875170453\n",
      "0.0471756530723\n",
      "0.049299826928\n",
      "0.0513419004343\n",
      "0.053285401837\n",
      "0.0551154887239\n",
      "0.0568189058992\n",
      "0.0583839398634\n",
      "0.0598003712488\n",
      "0.0610594263002\n",
      "0.062153728253\n",
      "0.0630772492559\n",
      "0.0638252633088\n",
      "0.0643943005293\n",
      "0.0647821029294\n",
      "0.0649875817706\n",
      "0.0650107764661\n",
      "0.0648528149191\n",
      "0.0645158751144\n",
      "0.0640031477223\n",
      "0.0633187994244\n",
      "0.0624679366298\n",
      "0.0614565692162\n",
      "0.0602915739049\n",
      "0.0589806568548\n",
      "0.0575323150485\n",
      "0.0559557960315\n",
      "0.054261055563\n",
      "0.0524587127381\n",
      "0.0505600021507\n",
      "0.0485767226786\n",
      "0.0465211824988\n",
      "0.044406139967\n",
      "0.0422447400395\n",
      "0.0400504459585\n",
      "0.037836965988\n",
      "0.0356181750513\n",
      "0.0334080312072\n",
      "0.0312204869956\n",
      "0.0290693957886\n",
      "0.0269684134036\n",
      "0.0249308953664\n",
      "0.0229697903545\n",
      "0.0210975305045\n",
      "0.0193259194291\n",
      "0.0176660189554\n",
      "0.0161280357675\n",
      "0.0147212093053\n",
      "0.0134537024337\n",
      "0.0123324965516\n",
      "0.0113632929409\n",
      "0.0105504222736\n",
      "0.00989676427257\n",
      "0.00940367956848\n",
      "0.00907095579434\n",
      "0.00889676991024\n",
      "0.00887766864185\n",
      "0.00900856874836\n",
      "0.00928277860232\n",
      "0.00969204226413\n",
      "0.0102266068704\n",
      "0.0108753137304\n",
      "0.0116257130443\n",
      "0.0124642016344\n",
      "0.0133761825214\n",
      "0.0143462446035\n",
      "0.0153583601233\n",
      "0.0163960970523\n",
      "0.0174428430066\n",
      "0.0184820368567\n",
      "0.0194974038213\n",
      "0.0204731895624\n",
      "0.0213943886415\n",
      "0.0222469626704\n",
      "0.0230180435921\n",
      "0.0236961177738\n",
      "0.0242711869688\n",
      "0.0247349027092\n",
      "0.0250806713042\n",
      "0.0253037273221\n",
      "0.0254011742111\n",
      "0.025371991525\n",
      "0.0252170090495\n",
      "0.0249388489344\n",
      "0.024541837706\n",
      "0.0240318907277\n",
      "0.0234163722837\n",
      "0.0227039349507\n",
      "0.0219043422937\n",
      "0.021028279157\n",
      "0.0200871539275\n",
      "0.0190928971193\n",
      "0.0180577604814\n",
      "0.0169941205728\n",
      "0.0159142904004\n",
      "0.0148303422889\n",
      "0.0137539446711\n",
      "0.0126962149769\n",
      "0.0116675902617\n",
      "0.0106777166957\n",
      "0.00973535851985\n",
      "0.00884832660533\n",
      "0.00802342631687\n",
      "0.00726642400508\n",
      "0.00658203112917\n",
      "0.00597390475112\n",
      "0.00544466294175\n",
      "0.00499591349733\n",
      "0.00462829427871\n",
      "0.00434152344873\n",
      "0.00413445789184\n",
      "0.00400515814617\n",
      "0.00395095825581\n",
      "0.0039685390535\n",
      "0.00405400350455\n",
      "0.00420295287621\n",
      "0.00441056263676\n",
      "0.00467165713139\n",
      "0.00498078222338\n",
      "0.0053322752252\n",
      "0.00572033157355\n",
      "0.00613906782205\n",
      "0.00658258063487\n",
      "0.00704500156231\n",
      "0.00752054746583\n",
      "0.00800356653464\n",
      "0.00848857989908\n",
      "0.00897031889895\n",
      "0.00944375810677\n",
      "0.0099041442399\n",
      "0.01034702112\n",
      "0.0107682508563\n",
      "0.0111640314398\n",
      "0.0115309109429\n",
      "0.0118657985185\n",
      "0.0121659723916\n",
      "0.0124290850306\n",
      "0.0126531656781\n",
      "0.0128366204124\n",
      "0.0129782298997\n",
      "0.0130771449876\n",
      "0.0131328802786\n",
      "0.0131453058129\n",
      "0.0131146369785\n",
      "0.0130414227588\n",
      "0.0129265324182\n",
      "0.0127711407213\n",
      "0.0125767117739\n",
      "0.0123449815692\n",
      "0.0120779393238\n",
      "0.0117778076812\n",
      "0.0114470218659\n",
      "0.0110882078694\n",
      "0.0107041597555\n",
      "0.0102978161746\n",
      "0.00987223618407\n",
      "0.00943057447868\n",
      "0.00897605614351\n",
      "0.00851195104962\n",
      "0.00804154802408\n",
      "0.00756812893501\n",
      "0.00709494284308\n",
      "0.00662518038097\n",
      "0.00616194853242\n",
      "0.00570824599125\n",
      "0.00526693928948\n",
      "0.00484073988999\n",
      "0.00443218244442\n",
      "0.00404360442025\n",
      "0.00367712730137\n",
      "0.00333463956445\n",
      "0.00301778162864\n",
      "0.0027279329674\n",
      "0.00246620155985\n",
      "0.00223341584327\n",
      "0.00203011930983\n",
      "0.00185656786707\n",
      "0.00171273005606\n",
      "0.00159829019065\n",
      "0.0015126544487\n",
      "0.00145495990997\n",
      "0.0014240864977\n",
      "0.00141867174079\n",
      "0.00143712823278\n",
      "0.00147766362265\n",
      "0.00153830293207\n",
      "0.00161691295425\n",
      "0.00171122845289\n",
      "0.00181887984594\n",
      "0.00193742202936\n",
      "0.00206436397138\n",
      "0.00219719868859\n",
      "0.0023334332028\n",
      "0.00247061807138\n",
      "0.00260637608543\n",
      "0.00273842973871\n",
      "0.00286462708645\n",
      "0.00298296563671\n",
      "0.00309161394708\n",
      "0.00318893063646\n",
      "0.00327348056394\n",
      "0.00334404797358\n",
      "0.0033996464553\n",
      "0.00343952562522\n",
      "0.00346317448383\n",
      "0.00347032146564\n",
      "0.00346093124843\n",
      "0.00343519844182\n",
      "0.0033935383244\n",
      "0.00333657484266\n",
      "0.00326512612487\n",
      "0.00318018779633\n",
      "0.00308291440973\n",
      "0.00297459932478\n",
      "0.0028566533843\n",
      "0.00273058274101\n",
      "0.00259796618797\n",
      "0.00246043233956\n",
      "0.00231963699614\n",
      "0.00217724100785\n",
      "0.00203488892937\n",
      "0.00189418873064\n",
      "0.00175669279827\n",
      "0.00162388042974\n",
      "0.00149714198833\n",
      "0.00137776485212\n",
      "0.00126692125539\n",
      "0.00116565808695\n",
      "0.00107488867688\n",
      "0.000995386572814\n",
      "588\n",
      "[[-0.3732905050049592, -0.3732905050049592, -0.3732905050049592, -0.3732905050049592], [-0.3057931437371135, -0.3057931437371135, -0.3057931437371135, -0.3057931437371135], [-0.21587158874219706, -0.21587158874219706, -0.21587158874219706, -0.21587158874219706], [-0.1084949194651662, -0.1084949194651662, -0.1084949194651662, -0.1084949194651662], [0.010403212254871898, 0.010403212254871898, 0.010403212254871898, 0.010403212254871898], [0.13425247672993218, 0.13425247672993218, 0.13425247672993218, 0.13425247672993218], [0.2562089438877406, 0.2562089438877406, 0.2562089438877406, 0.2562089438877406], [0.36953327993712737, 0.36953327993712737, 0.36953327993712737, 0.36953327993712737], [0.46796316402623167, 0.46796316402623167, 0.46796316402623167, 0.46796316402623167], [0.5460593450898016, 0.5460593450898016, 0.5460593450898016, 0.5460593450898016]]\n",
      "[0.5468947307775115, 0.5468947307775115, 0.5468947307775115, 0.5468947307775115]\n",
      "window size:  10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106e78190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def solve_problem(lenght, steps_count, window_size):\n",
    "    step_length = lenght / steps_count\n",
    "    #p = generate_sin_dots(1, 5, 0, steps_count, step_length);\n",
    "    \n",
    "    a = 0.2\n",
    "    b = 0.6\n",
    "    c = 0.05\n",
    "    d = 0.6\n",
    "    \n",
    "    \n",
    "    p = generate_complex_func_dots(a, b, c, d, steps_count, step_length)\n",
    "    longp = p\n",
    "    p = p[0: len(p) / 2]\n",
    "    new_steps_count = len(p)\n",
    "    p = zip(*p)\n",
    "    sample_size = new_steps_count - window_size\n",
    "    sample = []\n",
    "    answers = []\n",
    "    for i in range(0, sample_size) :\n",
    "        sample.append(p[1][i : i + window_size])\n",
    "        answers.append(p[1][i + window_size])\n",
    "    prc = two_layer_perceptrone(len(sample[0]), 4)\n",
    "    prc.teach(sample, answers, 0.001)\n",
    "    print prc.w_interm\n",
    "    print prc.w_out\n",
    "    plt.plot(p[0], p[1], 'r.')\n",
    "    newdots = list(p[1][len(list(p[1])) - window_size : len(list(p[1]))])\n",
    "    #newdots = list(p[1][0 : window_size])\n",
    "    print \"window size: \", window_size\n",
    "    x = 0.0\n",
    "    for i in range(0, steps_count * 2):\n",
    "        x = 0.0 + (len(p[1]) + i)*step_length\n",
    "        #x = (i + window_size)*step_length\n",
    "        y = prc.get_prophecy(newdots[len(newdots) - window_size: len(newdots)])\n",
    "        plt.plot(x, y, 'b.')\n",
    "        newdots.append(y)\n",
    "        \n",
    "    #print newdots\n",
    "        \n",
    "\n",
    "    maxvalue = abs(a)+abs(c)\n",
    "    plt.axis([0, x, -maxvalue*3, maxvalue*3])\n",
    "    \n",
    "    plt.show()\n",
    "     \n",
    "solve_problem(10*math.pi, 80, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
